{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:27.252656Z",
     "start_time": "2017-11-22T17:24:25.391452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:24:26 INFO:load utils\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "%run ../utils.ipynb\n",
    "from functools import partial\n",
    "\n",
    "root_dir = '/data/ai/data/kaggle_dog-breed-identification'\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "params_dir = os.path.join(root_dir, \"params\")\n",
    "grads_dir = os.path.join(root_dir, \"grads\")\n",
    "\n",
    "if not os.path.exists(params_dir):\n",
    "    os.mkdir(params_dir)\n",
    "if not os.path.exists(grads_dir):\n",
    "    os.mkdir(grads_dir)\n",
    "    \n",
    "demo=False\n",
    "first = False \n",
    "if demo:\n",
    "    # 注意：此处使用小数据集为便于网页编译。\n",
    "    input_dir = 'train_valid_test_tiny'\n",
    "    # 注意：此处相应使用小批量。对Kaggle的完整数据集可设较大的整数，例如128。\n",
    "    batch_size = 2\n",
    "else:\n",
    "    label_file = 'labels.csv'\n",
    "    train_dir = 'train'\n",
    "    test_dir = 'test'\n",
    "    input_dir = 'train_valid_test'\n",
    "    batch_size = 128\n",
    "    valid_ratio = 0.1\n",
    "    if first:\n",
    "        reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:27.309237Z",
     "start_time": "2017-11-22T17:24:27.254741Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "import mxnet.gluon as gl\n",
    "\n",
    "mean = np.array([ 0.39186783, 0.45182955, 0.47607605])\n",
    "std = np.array([ 0.26173923, 0.2573802, 0.26252426])\n",
    "\n",
    "random_shape = int(np.random.uniform() * 224 + 256)  \n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 96, 96), resize=random_shape,\n",
    "    #auglist = image.CreateAugmenter(data_shape=(3, 224, 224), resize=random_shape,\n",
    "                        rand_crop=True, rand_resize=True, rand_mirror=True,\n",
    "                        mean= mean,\n",
    "                        std= std,\n",
    "                        brightness=0, contrast=0,\n",
    "                        saturation=0, hue=0,\n",
    "                        pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 96, 96), \n",
    "    #auglist = image.CreateAugmenter(data_shape=(3, 224, 224), \n",
    "                        mean=mean,\n",
    "                        std=std)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:36.712366Z",
     "start_time": "2017-11-22T17:24:35.102278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 128\n",
      "(128,) <class 'numpy.float32'>\n",
      "(128, 3, 96, 96) <class 'numpy.float32'>\n",
      "total train data 9502\n"
     ]
    }
   ],
   "source": [
    "input_str = data_dir + '/' + input_dir + '/'\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1, transform=transform_train)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1, transform=transform_test)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid', flag=1, transform=transform_train)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1, transform=transform_test)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "\n",
    "\n",
    "print(\"batch size\", batch_size)\n",
    "data, label = list(itertools.islice(train_data, 1))[0]\n",
    "\n",
    "#ctx = [mx.gpu(i) for i in range(1)]\n",
    "ctx = mx.cpu(0)\n",
    "label = label.as_in_context(ctx)\n",
    "print(label.shape, label.dtype)\n",
    "data = data.as_in_context(ctx)\n",
    "print(data.shape, data.dtype)\n",
    "print(\"total train data\", len(train_ds))\n",
    "\n",
    "for img, _ in train_ds:\n",
    "    #print(img)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:45.007607Z",
     "start_time": "2017-11-22T17:24:45.004513Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for _, i in net.features.collect_params().items():\n",
    "#    i.lr_mult = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:45.972424Z",
     "start_time": "2017-11-22T17:24:45.719045Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_loss(data, net, ctx):\n",
    "    loss = 0.0\n",
    "    for feas, label in data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(feas.as_in_context(ctx))\n",
    "        cross_entropy = softmax_cross_entropy(output, label)\n",
    "        loss += nd.mean(cross_entropy).asscalar()\n",
    "    return loss / len(data)\n",
    "\n",
    "def get_acc(output, label):\n",
    "    #pred = output.argmax(1, keepdims=True)\n",
    "    pred = nd.argmax(output , axis=1)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def train(model_name, net, train_data, valid_data, trainer, ctx, start_epoch=0, end_epoch=3, lr_period=80, lr_decay=0.1):\n",
    "    print(\"model\", model_name)\n",
    "    if trainer is None or net is None:\n",
    "        print(\"Need trainer\")\n",
    "        return\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        print(\"epoch\", epoch, \"learning rate\", trainer.learning_rate)\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            batch_size = label.shape[0]\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            #print(\"output\", nd.argmax(output, axis=1))\n",
    "            #print(\"label\", label)\n",
    "            correct += get_acc(output, label)\n",
    "            total += batch_size\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        print('total', total, 'batches', len(train_data))\n",
    "        if valid_data is not None:\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            valid_loss = 0\n",
    "            for data, label in valid_data:\n",
    "                batch_size = data.shape[0]\n",
    "                data = data.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "                valid_loss += nd.mean(loss).asscalar()\n",
    "                valid_correct += get_acc(output, label)\n",
    "                valid_total += batch_size\n",
    "            valid_acc = valid_correct / valid_total\n",
    "        #if valid_data is not None:\n",
    "        #    valid_loss = get_loss(valid_data, net, ctx)\n",
    "        #    epoch_str = (\"Epoch %d. Train loss: %f, Valid loss: %f, Train accuracy: %.4f, \"\n",
    "        #                 % (epoch, train_loss / len(train_data), valid_loss , correct / total))\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, Valid loss: %f, Train accuracy: %.4f, Valid accuracy: %.4f\"\n",
    "                         % (epoch, train_loss / len(train_data), valid_loss / len(valid_data), correct / total, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, \"\n",
    "                         % (epoch, train_loss / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        if (epoch % 2 == 0):\n",
    "            file = \"m_%s_e_%s_v_%s_t_%s_lr_%s\" % (model_name, epoch, valid_loss / len(valid_data), train_loss/len(train_data), trainer.learning_rate)\n",
    "            params_file = os.path.join(os.path.join(params_dir, model_name, file + \".params\"))\n",
    "            net.save_params(params_file)\n",
    "            #net.collect_params().save(params_file)\n",
    "            grads_file =  os.path.join(grads_dir, model_name, file + \".h5\")\n",
    "            print(grads_file)\n",
    "            write_net_params(net, grads_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:53.448347Z",
     "start_time": "2017-11-22T17:24:53.429912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor learning_rate in [1e-2, 1e-3, 1e-4]:\\n    pretrained_net[model] = gl.model_zoo.vision.resnet152_v1(pretrained=True)\\n    finetune_net[model] = gl.model_zoo.vision.resnet152_v1(classes=120)\\n    finetune_net[model].features = pretrained_net[model].features\\n    finetune_net[model].classifier.initialize(init.Xavier())\\n    \\n    trainer = gluon.Trainer(finetune_net[model].collect_params(), 'adam', {'learning_rate': learning_rate, 'wd': weight_decay})\\n    train_lr(net=finetune_net[model], trainer=trainer)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ctx = utils.try_gpu()\n",
    "ctx=mx.gpu()\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-4\n",
    "lr_period = 200\n",
    "lr_decay = 0.1\n",
    "\n",
    "#net = get_net(ctx)\n",
    "pretrained_net=dict()\n",
    "finetune_net=dict()\n",
    "\n",
    "#for _, i in net.features.collect_params().items():\n",
    "#   i.lr_mult = 0.1\n",
    "\n",
    "#trainer = gluon.Trainer( net.collect_params(), 'sgd', {'learning_rate': learning_rate, 'momentum': 0.9, 'wd': weight_decay})\n",
    "train_lr =  partial(train, train_data=train_data, valid_data=valid_data, ctx=ctx, lr_decay=lr_decay)\n",
    "\"\"\"\n",
    "for learning_rate in [1e-2, 1e-3, 1e-4]:\n",
    "    pretrained_net[model] = gl.model_zoo.vision.resnet152_v1(pretrained=True)\n",
    "    finetune_net[model] = gl.model_zoo.vision.resnet152_v1(classes=120)\n",
    "    finetune_net[model].features = pretrained_net[model].features\n",
    "    finetune_net[model].classifier.initialize(init.Xavier())\n",
    "    \n",
    "    trainer = gluon.Trainer(finetune_net[model].collect_params(), 'adam', {'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "    train_lr(net=finetune_net[model], trainer=trainer)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:25:02.120335Z",
     "start_time": "2017-11-22T17:25:01.204219Z"
    }
   },
   "outputs": [],
   "source": [
    "res152_v1_model=\"resnet152_v1\"\n",
    "dense161_model = \"densenet161\"\n",
    "vgg16_bn_model = \"vgg16_bn\"\n",
    "inception_v3_model = \"inception_v3\"\n",
    "res50_v2_model=\"resnet50_v2\"\n",
    "res18_v2_model=\"resnet18_v2\"\n",
    "pretrained_net=dict()\n",
    "#pretrained_net[res18_v2_model] = gl.model_zoo.vision.resnet18_v2(prefix=res18_v2_model + \"_\", pretrained=True)\n",
    "#pretrained_net[res50_v2_model] = gl.model_zoo.vision.resnet50_v2(pretrained=True)\n",
    "#print(pretrained_net[res50_v2_model].features[1].weight.data()[0][0])\n",
    "\n",
    "pretrained_net[res152_v1_model] = gl.model_zoo.vision.resnet152_v1(prefix=res152_v1_model + \"_\", pretrained=True)\n",
    "#pretrained_net[dense161_model] = gl.model_zoo.vision.densenet161(pretrained=True)\n",
    "#pretrained_net[vgg16_bn_model] = gl.model_zoo.vision.vgg16_bn(pretrained=True)\n",
    "#pretrained_net[inception_v3_model] = gl.model_zoo.vision.inception_v3(pretrained=True)\n",
    "\n",
    "\"\"\"\n",
    "# Train indivisually\n",
    "    if (model == res152_v1_model):\n",
    "        finetune_net = gl.model_zoo.vision.resnet152_v1(classes=120, ctx=ctx)\n",
    "    if (model == res50_v2_model):\n",
    "        finetune_net = gl.model_zoo.vision.resnet50_v2(classes=120, ctx=ctx)\n",
    "    if (model == vgg16_bn_model):\n",
    "        finetune_net = gl.model_zoo.vision.vgg16_bn(classes=120, ctx=ctx)\n",
    "    if (model == dense161_model):\n",
    "        finetune_net = gl.model_zoo.vision.dense161(classes=120, ctx=ctx)\n",
    "    if (model == inception_v3_model):\n",
    "        finetune_net = gl.model_zoo.vision.inception_v3(classes=120, ctx=ctx)\n",
    "        \n",
    "    finetune_net.features = pretrained_net[model].features\n",
    "    finetune_net.features = pretrained_net[model].features\n",
    "    finetune_net.classifier.initialize(init.Xavier(), ctx=ctx)\n",
    "    return finetune_net\n",
    "\"\"\"\n",
    "# Train togeter\n",
    "class AllInOneModel(nn.HybridBlock):\n",
    "    def __init__(self, name, networks, ** kwargs):\n",
    "        print(\"kwargs\", kwargs)\n",
    "        super(AllInOneModel, self).__init__(**kwargs)\n",
    "        self.networks = networks\n",
    "        self.dense_adapters = dict()\n",
    "        for name in networks:\n",
    "            #print(name, networks[name].features)\n",
    "            #print(name, networks[name].features[-1][-1])\n",
    "            #print(name, networks[name].features[-1][-1].body[-1])\n",
    "            #self.dense_adapters[name] = nn.Dense(128, activation=\"relu\") \n",
    "            pass\n",
    "        with self.name_scope():\n",
    "            net = self.classifier = nn.HybridSequential()\n",
    "            with net.name_scope():\n",
    "                net.add(nn.Dense(256, activation='relu'))\n",
    "                net.add(nn.Dropout(0.5))\n",
    "                net.add(nn.Dense(120))\n",
    "                \n",
    "    def hybrid_forward(self, F, x):\n",
    "        print(\"hybrid\")\n",
    "        #   net.add(nd.concatenate(features, axis=-1))\n",
    "        #for i, network in enumerate([self.networks[name].features for name in self.networks.keys() if name != dense161_model]):\n",
    "        for i, network in enumerate(self.networks):\n",
    "            #print(\"x\", x.shape)\n",
    "            #print(network)\n",
    "            nt = network(x)\n",
    "            # reshape to (batch, )\n",
    "            #nt = nt.reshape(nt.shape[:2])\n",
    "            #print(\"nt\", nt.shape)\n",
    "            if i == 0:\n",
    "                nts = nt\n",
    "            else:\n",
    "                nts = mx.nd.concat(nts, nt, dim=1)\n",
    "                #print(\"nts[]\", nts.shape)\n",
    "                \n",
    "        out = nts\n",
    "        # print(\"nts\", nts.shape)\n",
    "        out = self.net(out)\n",
    "        return out \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"only forward\")\n",
    "        #   net.add(nd.concatenate(features, axis=-1))\n",
    "        #for i, network in enumerate([self.networks[name].features for name in self.networks.keys() if name != dense161_model]):\n",
    "        for i, network in enumerate(self.networks):\n",
    "            #print(\"x\", x.shape)\n",
    "            #print(network)\n",
    "            nt = network(x)\n",
    "            # reshape to (batch, )\n",
    "            #nt = nt.reshape(nt.shape[:2])\n",
    "            #print(\"nt\", nt.shape)\n",
    "            if i == 0:\n",
    "                nts = nt\n",
    "            else:\n",
    "                nts = mx.nd.concat(nts, nt, dim=1)\n",
    "                #print(\"nts[]\", nts.shape)\n",
    "                \n",
    "        out = nts\n",
    "        # print(\"nts\", nts.shape)\n",
    "        out = self.classifier(out)\n",
    "        return out \n",
    "\n",
    "def get_net(name):\n",
    "    networks = [pretrained_net[name].features for name in pretrained_net.keys() if name != dense161_model]\n",
    "    net = AllInOneModel(name, networks, prefix=name)\n",
    "    net.classifier.initialize(ctx=ctx, init=mx.init.Xavier())\n",
    "    net.hybridize()\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-22T17:25:09.956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model resnet152_v1\n",
      "epoch 0 learning rate 0.0001\n",
      "total 9502 batches 75\n",
      "Epoch 0. Train loss: 3.260848, Valid loss: 2.594458, Train accuracy: 0.2784, Valid accuracy: 0.3583Time 00:02:38, lr 0.0001\n",
      "/data/ai/data/kaggle_dog-breed-identification/grads/resnet152_v1/m_resnet152_v1_e_0_v_2.59445750713_t_3.26084752083_lr_0.0001.h5\n",
      "epoch 1 learning rate 0.0001\n",
      "total 9502 batches 75\n",
      "Epoch 1. Train loss: 1.805435, Valid loss: 2.331380, Train accuracy: 0.5304, Valid accuracy: 0.3972Time 00:02:42, lr 0.0001\n",
      "epoch 2 learning rate 0.0001\n",
      "total 9502 batches 75\n",
      "Epoch 2. Train loss: 1.561534, Valid loss: 2.199375, Train accuracy: 0.5838, Valid accuracy: 0.4278Time 00:02:39, lr 0.0001\n",
      "/data/ai/data/kaggle_dog-breed-identification/grads/resnet152_v1/m_resnet152_v1_e_2_v_2.19937547048_t_1.56153410435_lr_0.0001.h5\n",
      "epoch 3 learning rate 0.0001\n",
      "total 9502 batches 75\n",
      "Epoch 3. Train loss: 1.394785, Valid loss: 2.236949, Train accuracy: 0.6226, Valid accuracy: 0.4097Time 00:02:40, lr 0.0001\n",
      "epoch 4 learning rate 0.0001\n",
      "total 9502 batches 75\n",
      "Epoch 4. Train loss: 1.261047, Valid loss: 2.292378, Train accuracy: 0.6587, Valid accuracy: 0.4250Time 00:02:39, lr 0.0001\n",
      "/data/ai/data/kaggle_dog-breed-identification/grads/resnet152_v1/m_resnet152_v1_e_4_v_2.29237782955_t_1.26104678392_lr_0.0001.h5\n",
      "epoch 5 learning rate 0.0001\n",
      "total 9502 batches 75\n",
      "Epoch 5. Train loss: 1.162773, Valid loss: 2.219330, Train accuracy: 0.6803, Valid accuracy: 0.4264Time 00:02:40, lr 9e-05\n",
      "epoch 6 learning rate 9e-05\n",
      "total 9502 batches 75\n",
      "Epoch 6. Train loss: 1.085286, Valid loss: 2.270430, Train accuracy: 0.7004, Valid accuracy: 0.4333Time 00:02:39, lr 9e-05\n",
      "/data/ai/data/kaggle_dog-breed-identification/grads/resnet152_v1/m_resnet152_v1_e_6_v_2.27043048541_t_1.08528557618_lr_9e-05.h5\n",
      "epoch 7 learning rate 9e-05\n",
      "total 9502 batches 75\n",
      "Epoch 7. Train loss: 1.011257, Valid loss: 2.130402, Train accuracy: 0.7168, Valid accuracy: 0.4375Time 00:02:40, lr 9e-05\n",
      "epoch 8 learning rate 9e-05\n",
      "total 9502 batches 75\n",
      "Epoch 8. Train loss: 0.937212, Valid loss: 2.208469, Train accuracy: 0.7436, Valid accuracy: 0.4389Time 00:02:39, lr 9e-05\n",
      "/data/ai/data/kaggle_dog-breed-identification/grads/resnet152_v1/m_resnet152_v1_e_8_v_2.20846947034_t_0.937212486267_lr_9e-05.h5\n",
      "epoch 9 learning rate 9e-05\n"
     ]
    }
   ],
   "source": [
    "#model = res50_v2_model\n",
    "model_name =  \"_\".join(pretrained_net.keys()) \n",
    "#net=get_net(model_name)\n",
    "#net.hybridize()\n",
    "params_model_dir = os.path.join(params_dir, model_name)\n",
    "grads_model_dir = os.path.join(grads_dir, model_name)\n",
    "if not os.path.exists(params_model_dir):\n",
    "    os.mkdir(params_model_dir)\n",
    "if not os.path.exists(grads_model_dir):\n",
    "    os.mkdir(grads_model_dir)\n",
    " \n",
    "#file= \"m_resnet152_v1_e_18_v_2.81106563409_t_3.09018225034_lr_0.0001.params\"\n",
    "#file = \"m_resnet152_v1_e_188_v_2.22175089518_t_0.272732603749_lr_1e-05.params\"\n",
    "#file = \"m_resnet152_v1_e_248_v_2.21313051383_t_0.274849429925_lr_1.0000000000000002e-06.params\"\n",
    "#file = \"m_resnet152_v1_e_250_v_2.80912895997_t_3.26908252398_lr_0.0001.params\"\n",
    "#file = \"m_resnet50_v2_e_113_v_2.66837227345_t_2.69866662979_lr_0.001.params\"\n",
    "#file = \"m_resnet18_v2_e_6_v_1.63639547427_t_2.00945680173_lr_0.01.params\"\n",
    "file = \"m_resnet18_v2_e_1008_v_1.50832762321_t_1.32757116804_lr_0.1.params\"\n",
    "params_file = os.path.join(params_model_dir, file)\n",
    "#print(params_file)\n",
    "\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "#finetune_net = models.resnet18_v2(prefix=res18_v2_model + \"_\", classes=120)\n",
    "finetune_net = models.resnet152_v1(prefix=res152_v1_model + \"_\", classes=120)\n",
    "finetune_net.features = pretrained_net[res152_v1_model].features\n",
    "finetune_net.classifier.initialize(init.Xavier())\n",
    "net=finetune_net\n",
    "#print(net.collect_params())\n",
    "#net.load_params(params_file, ctx=ctx)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "net.hybridize()\n",
    "\n",
    "start_epoch= 0 \n",
    "end_epoch=100\n",
    "learning_rate = 1e-2\n",
    "lr_period = 200\n",
    "weight_decay= 5e-4\n",
    "batch_size=128\n",
    "#trainer = gluon.Trainer( net.collect_params(), 'adam', {'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "#trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "lr_sch = mx.lr_scheduler.FactorScheduler(step=400, factor=0.9)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam',  {'learning_rate': 1e-4, 'wd': 1e-5, 'lr_scheduler': lr_sch})\n",
    "train_lr(model_name=model_name, net=net, trainer=trainer, start_epoch=start_epoch, \n",
    "         end_epoch=end_epoch, lr_period=lr_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:05.150795Z",
     "start_time": "2017-11-22T17:23:39.910Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "nd.waitall??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:05.151634Z",
     "start_time": "2017-11-22T17:23:39.911Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for data, label in test_data:\n",
    "    output = nd.softmax(net(data.as_in_context(ctx)))\n",
    "    outputs.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, outputs):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:05.152526Z",
     "start_time": "2017-11-22T17:23:39.911Z"
    }
   },
   "outputs": [],
   "source": [
    "get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:05.153328Z",
     "start_time": "2017-11-22T17:23:39.912Z"
    }
   },
   "outputs": [],
   "source": [
    "finetune_net.classifier.initialize(init.Xavier())\n",
    "net=finetune_net\n",
    "#net.save_params(\"/tmp/t\")\n",
    "\n",
    "print(finetune_net.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:05.154132Z",
     "start_time": "2017-11-22T17:23:39.913Z"
    }
   },
   "outputs": [],
   "source": [
    "print(finetune_net.classifier.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T17:24:05.154953Z",
     "start_time": "2017-11-22T17:23:39.914Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
